{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data file\n",
    "data_circle = pd.read_csv('circle.csv', sep=',',delimiter=None, header=None)\n",
    "data_cross = pd.read_csv('cross.csv', sep=',', delimiter=None, header=None)\n",
    "data_square = pd.read_csv('square.csv', sep=',', delimiter=None, header=None)\n",
    "data_triangle = pd.read_csv('triangle.csv', sep=',', delimiter=None, header=None)\n",
    "\n",
    "#Join data and shuffle randomly\n",
    "data_combined = pd.concat([data_circle, data_cross, data_square, data_triangle])\n",
    "data_combined.to_csv( index=False )\n",
    "data_combined = data_combined.reset_index()\n",
    "data_suffled = data_combined.sample(frac=1).reset_index(drop=True)\n",
    "data_suffled = data_suffled.drop('index',1)\n",
    "\n",
    "#Write csv back to the file\n",
    "#data_suffled.to_csv(\"combined_data.csv\", sep = ',')\n",
    "\n",
    "#Processing data as X_all and Y_all\n",
    "X_all = data_suffled.iloc[:,0:784]\n",
    "Y_all = data_suffled.iloc[:,784]\n",
    "\n",
    "\n",
    "#Converting Y_all to hot vectors\n",
    "#converter = lambda x: ord(x)-ord('A')\n",
    "#Y_all = list(map(converter,Y_all.values.T.tolist()))\n",
    "Y_all = list(Y_all.values.T.tolist())\n",
    "Y_all = pd.get_dummies(Y_all).values\n",
    "\n",
    "\n",
    "X_all = pd.DataFrame(X_all)\n",
    "Y_all = pd.DataFrame(Y_all)\n",
    "#print(X_all)\n",
    "#print(Y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 784)\n",
      "(409, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_all.shape)\n",
    "print(Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 1024, 100, 150, 20, 4]\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = Y_all.shape[1]\n",
    "INPUT_SIZE = X_all.shape[1]\n",
    "BATCH_SIZE = 20\n",
    "NR_STEP = int(X_all.shape[0]/BATCH_SIZE)\n",
    "NR_EPOCH = 10\n",
    "\n",
    "layer_info = [INPUT_SIZE, 1024, 100, 150, 20, NUM_CLASSES]\n",
    "layer_size = len(layer_info)\n",
    "\n",
    "logs_path = '/tf_log/'\n",
    "\n",
    "print(layer_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_graph(X, layer_info):\n",
    "    #Define weight, bias and output under different namescope\n",
    "    layer_input = X\n",
    "    for i in range(1,len(layer_info)-1):\n",
    "        with tf.name_scope('layer_'+str(i)):\n",
    "            W = tf.Variable(tf.truncated_normal([layer_info[i-1],layer_info[i]], \n",
    "                                                stddev=1.0/math.sqrt(float(layer_info[i-1]))),\n",
    "                           name = 'weight_'+str(i))\n",
    "            tf.summary.histogram('weight_histogram_'+str(i), W)\n",
    "            b = tf.Variable(tf.zeros([layer_info[i]]),\n",
    "                           name = 'bias_'+str(i))\n",
    "            tf.summary.histogram('bias_histogram_'+str(i), b)\n",
    "            layer_output = tf.nn.relu(tf.matmul(layer_input, W) + b)\n",
    "            \n",
    "            #print(layer_output)\n",
    "            \n",
    "            layer_input = layer_output\n",
    "    \n",
    "    nr_layer = len(layer_info)\n",
    "    with tf.name_scope('layer_'+str(nr_layer-1)):\n",
    "        W = tf.Variable(tf.truncated_normal([layer_info[nr_layer-2],layer_info[nr_layer-1]], \n",
    "                                            stddev=1.0/math.sqrt(float(layer_info[nr_layer-2]))),\n",
    "                       name = 'weight_'+str(nr_layer-1))\n",
    "        tf.summary.histogram('weight_histogram_'+str(nr_layer-1), W)\n",
    "        b = tf.Variable(tf.zeros([layer_info[nr_layer-1]]),\n",
    "                        name = 'bias_'+str(nr_layer-1))\n",
    "        tf.summary.histogram('bias_histogram_'+str(nr_layer-1), b)\n",
    "        layer_output = tf.matmul(layer_input, W) + b\n",
    "        softmax_output = tf.nn.softmax(layer_output, name='OUTPUT')\n",
    "\n",
    "    return layer_output, softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training graph construction\n",
    "\n",
    "def train_graph(logit, Y, learning_rate):\n",
    "    print(logit.shape)\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logit, name = 'xentropy')\n",
    "        loss = tf.reduce_mean(softmax_entropy, name = 'loss')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        nr_correct = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(nr_correct, tf.float32))\n",
    "        tf.summary.scalar('accuracy_mean', accuracy)\n",
    "    \n",
    "    return loss, train_op, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving graph to protobuf requires three things:\n",
    "#1. Saving graph defenition (format: pbtxt)\n",
    "#2. Saving weights checkpoint (format: chkp)\n",
    "#3. Freezing graph along with weight (format: pb)\n",
    "\n",
    "def export_model(input_name, output_name):\n",
    "    freeze_graph.freeze_graph('out/model_def.pbtxt', None, False, 'out/model_checkpoint.chkp', output_name, \n",
    "                             \"save/restore_all\", \"save/Const:0\", 'out/model_frozen.pb', True, \"\")\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/model_frozen.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "                        input_graph_def, [input_name], [output_name],tf.float32.as_datatype_enum)\n",
    "    with tf.gfile.FastGFile('out/optimized_model.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToSring())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4)\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow full graph construction\n",
    "\n",
    "model_graph = tf.Graph()\n",
    "with model_graph.as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        X = tf.placeholder(tf.float32, shape = [None, INPUT_SIZE], name = 'X_INPUT')\n",
    "        Y = tf.placeholder(tf.float32, shape = [None, NUM_CLASSES], name = 'Y_INPUT')\n",
    "\n",
    "    logits, output = inference_graph(X, layer_info)\n",
    "    loss, train_op, accuracy = train_graph(logits, Y, 0.01)\n",
    "\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 0, step= 0, accuracy= 0.30 loss= 1.40\n",
      "Epoch= 1, step= 0, accuracy= 0.30 loss= 1.31\n",
      "Epoch= 2, step= 0, accuracy= 0.30 loss= 1.24\n",
      "Epoch= 3, step= 0, accuracy= 0.30 loss= 1.17\n",
      "Epoch= 4, step= 0, accuracy= 0.30 loss= 1.09\n",
      "Epoch= 5, step= 0, accuracy= 0.30 loss= 1.02\n",
      "Epoch= 6, step= 0, accuracy= 0.40 loss= 0.91\n",
      "Epoch= 7, step= 0, accuracy= 0.55 loss= 0.76\n",
      "Epoch= 8, step= 0, accuracy= 0.75 loss= 0.64\n",
      "Epoch= 9, step= 0, accuracy= 0.90 loss= 0.48\n",
      "Epoch= 10, step= 0, accuracy= 0.90 loss= 0.35\n",
      "Epoch= 11, step= 0, accuracy= 0.95 loss= 0.25\n",
      "Epoch= 12, step= 0, accuracy= 1.00 loss= 0.20\n",
      "Epoch= 13, step= 0, accuracy= 1.00 loss= 0.16\n",
      "Epoch= 14, step= 0, accuracy= 1.00 loss= 0.13\n",
      "Epoch= 15, step= 0, accuracy= 1.00 loss= 0.12\n",
      "Epoch= 16, step= 0, accuracy= 1.00 loss= 0.10\n",
      "Epoch= 17, step= 0, accuracy= 1.00 loss= 0.10\n",
      "Epoch= 18, step= 0, accuracy= 1.00 loss= 0.09\n",
      "Epoch= 19, step= 0, accuracy= 1.00 loss= 0.08\n",
      "Epoch= 20, step= 0, accuracy= 1.00 loss= 0.08\n",
      "Epoch= 21, step= 0, accuracy= 1.00 loss= 0.07\n",
      "Epoch= 22, step= 0, accuracy= 1.00 loss= 0.06\n",
      "Epoch= 23, step= 0, accuracy= 1.00 loss= 0.06\n",
      "Epoch= 24, step= 0, accuracy= 1.00 loss= 0.05\n",
      "Epoch= 25, step= 0, accuracy= 1.00 loss= 0.05\n",
      "Epoch= 26, step= 0, accuracy= 1.00 loss= 0.04\n",
      "Epoch= 27, step= 0, accuracy= 1.00 loss= 0.04\n",
      "Epoch= 28, step= 0, accuracy= 1.00 loss= 0.03\n",
      "Epoch= 29, step= 0, accuracy= 1.00 loss= 0.03\n",
      "Epoch= 30, step= 0, accuracy= 1.00 loss= 0.03\n",
      "Epoch= 31, step= 0, accuracy= 1.00 loss= 0.02\n",
      "Epoch= 32, step= 0, accuracy= 1.00 loss= 0.02\n",
      "Epoch= 33, step= 0, accuracy= 1.00 loss= 0.02\n",
      "Epoch= 34, step= 0, accuracy= 1.00 loss= 0.02\n",
      "Epoch= 35, step= 0, accuracy= 1.00 loss= 0.02\n",
      "Epoch= 36, step= 0, accuracy= 1.00 loss= 0.02\n",
      "Epoch= 37, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 38, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 39, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 40, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 41, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 42, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 43, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 44, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 45, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 46, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 47, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 48, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 49, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 50, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 51, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 52, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 53, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 54, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 55, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 56, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 57, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 58, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 59, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 60, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 61, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 62, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 63, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 64, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 65, step= 0, accuracy= 1.00 loss= 0.01\n",
      "Epoch= 66, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 67, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 68, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 69, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 70, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 71, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 72, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 73, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 74, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 75, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 76, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 77, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 78, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 79, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 80, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 81, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 82, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 83, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 84, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 85, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 86, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 87, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 88, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 89, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 90, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 91, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 92, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 93, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 94, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 95, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 96, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 97, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 98, step= 0, accuracy= 1.00 loss= 0.00\n",
      "Epoch= 99, step= 0, accuracy= 1.00 loss= 0.00\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "\n",
    "with tf.Session(graph = model_graph) as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    #To save graph defenition\n",
    "    tf.train.write_graph(sess.graph_def, 'out','model_def.pbtxt' , True)\n",
    "    \n",
    "    for epoch in range(0, NR_EPOCH):\n",
    "        for step in range(0, NR_STEP):\n",
    "            feed_X = X_all.iloc[step*BATCH_SIZE:(step+1)*BATCH_SIZE, :]\n",
    "            feed_Y = Y_all.iloc[step*BATCH_SIZE:(step+1)*BATCH_SIZE, :]\n",
    "            _, step_accuracy, step_loss, summary = sess.run([train_op, accuracy, loss, merged_summary_op],\n",
    "                                                  feed_dict={X:feed_X, Y:feed_Y})\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                print('Epoch= %d, step= %d, accuracy= %.2f loss= %.2f' % (epoch, step, step_accuracy, step_loss))\n",
    "    #For saving the trained model's weight in checkpoint\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, 'out/' + 'model_checkpoint.chkp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input checkpoint 'out/model_checkpoint.chkp' doesn't exist!\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "out/model_frozen.pb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-893cb0a9c861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexport_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_INPUT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OUTPUT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2dcacc41f4e1>\u001b[0m in \u001b[0;36mexport_model\u001b[0;34m(input_name, output_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out/model_frozen.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0minput_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n\u001b[1;32m     13\u001b[0m                         input_graph_def, [input_name], [output_name],tf.float32.as_datatype_enum)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 78\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: out/model_frozen.pb"
     ]
    }
   ],
   "source": [
    "export_model('X_INPUT', 'OUTPUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
